{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "import gensim as gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from math import *\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from string import *\n",
    "import gensim.models\n",
    "from sklearn import linear_model\n",
    "from sklearn import neighbors\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "from sklearn import neural_network\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "import io\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the pre-tarined weights:\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization:\n",
    "\n",
    "def get_non_alphanumeric_chars(input_text):\n",
    "    \"\"\"\n",
    "        Returns the non alphanumeric unique characters from a corpus.\n",
    "        Input: \n",
    "            * input_text:   string\n",
    "        Output:\n",
    "            * non_alphanumeric_unique: list of non alphanumeric characters\n",
    "    \"\"\"     \n",
    "    ###Start code here\n",
    "    \n",
    "    #STEP1 set character_list variable to a list of characters in the input_text \n",
    "    character_list = list(input_text)\n",
    "    \n",
    "    #STEP2 store all non_alphanumeric characters from the character_list!\n",
    "    non_alphanumeric = []\n",
    "    for s in character_list:\n",
    "        if (s.isalnum()==False):\n",
    "            non_alphanumeric.append(s)\n",
    "            #character_list.delete(s)\n",
    "    \n",
    "    #STEP3 keep only the unique characters (hint: unique_values = list(set(duplicated_values))\n",
    "    non_alphanumeric_unique = list(set(non_alphanumeric))\n",
    "    \n",
    "    ###End code here \n",
    "    \n",
    "    return non_alphanumeric_unique\n",
    "\n",
    "def tokenizer(input_text):\n",
    "    \"\"\"\n",
    "        Transforms strings to tokens. The tokens should appear in the same order as they are in the string.\n",
    "        Input: \n",
    "            * input_text:   string\n",
    "        Output:\n",
    "            * tokens: list of tokens\n",
    "    \"\"\"    \n",
    "\n",
    "    \n",
    "    ###Start code here\n",
    "    #STEP0 extract non alphanumeric characters! Use your function above!\n",
    "    non_alphanumeric_unique = get_non_alphanumeric_chars(input_text)\n",
    "    \n",
    "    #STEP1 replace all non alphanumeric characters to ' ' whitespace.\n",
    "    replaced_input = input_text\n",
    "    for s in replaced_input:\n",
    "        if (s in non_alphanumeric_unique):\n",
    "            replaced_input=replaced_input.replace(s,\" \")\n",
    "    \n",
    "    #STEP2 split the input_text on all the non_alphanumeric_unique characters and store then in the tokens variable\n",
    "    tokens = replaced_input.split()\n",
    "    \n",
    "    ###End code here    \n",
    "    \n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train corpus:\n",
    "corpusfile=open('./trainemoji.txt', 'r', encoding='utf-8')\n",
    "lines=corpusfile.readlines()\n",
    "corpusfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we',\n",
       " 'have',\n",
       " 'to',\n",
       " 'hurt',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'know',\n",
       " 'fall',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'grow',\n",
       " 'lose',\n",
       " 'in',\n",
       " 'order',\n",
       " 'to',\n",
       " 'gain',\n",
       " 'because',\n",
       " 'all',\n",
       " 'of',\n",
       " 'life',\n",
       " 's',\n",
       " 'lessons',\n",
       " 'are',\n",
       " 'taught',\n",
       " 'through',\n",
       " 'pain']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example for a tokenized sentence:\n",
    "tokenizer(lines[3][1:].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorization of sentences:\n",
    "def vectorize(filename,model):\n",
    "    file=open(filename, \"r\", encoding=\"utf-8\")\n",
    "    lines=file.readlines()\n",
    "    file.close()\n",
    "    VECTORS=np.empty([len(lines),300])\n",
    "    for IDX in range(len(lines)):\n",
    "        W=np.zeros(300)\n",
    "        N=int(0)\n",
    "        for idx in range(len(tokenizer(lines[IDX][1:].lower()))):\n",
    "            s=tokenizer(lines[IDX][1:].lower())[idx]\n",
    "            if s in model:\n",
    "                W+=np.array(model.get_vector(s))\n",
    "                N+=1\n",
    "        W=W/N\n",
    "        for i in range(len(W)):\n",
    "            VECTORS[IDX][i]=W[i]\n",
    "    return (VECTORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#labels of senteces:\n",
    "def labelize(filename, model):\n",
    "    with io.open(filename, 'r', encoding='utf-8') as corpusfile:\n",
    "        lines=corpusfile.readlines()\n",
    "    corpusfile.close()\n",
    "    lineslabels=np.empty(len(lines))\n",
    "    for IDX in range(len(lines)):\n",
    "        lineslabels[IDX]=float(str(tokenizer(lines[IDX].lower())[0]))\n",
    "        \n",
    "    return lineslabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
      "            max_depth=None, max_features=20, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=50, n_jobs=20,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "#First, let's try using random forests.\n",
    "classify=RandomForestClassifier(n_estimators=50, n_jobs=20, max_features=20, criterion='entropy')\n",
    "print(classify.fit(vectorize('./trainemoji.txt',model), labelize('./trainemoji.txt',model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "testvectors=vectorize('./testemoji.txt',model)\n",
    "testlabels=labelize('./testemoji.txt',model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "[2. 2. 3. 0. 0. 2. 0. 3. 1. 0. 2. 1. 2. 1. 0. 2. 0. 1. 3. 2. 2. 2. 3. 0.\n",
      " 1. 1. 0. 0. 3. 0. 0. 3. 3. 1. 3. 0. 0. 1. 3. 1. 2. 1. 1. 0. 2. 1.]\n",
      "Fitted:\n",
      "\n",
      "[1. 1. 0. 0. 0. 2. 0. 3. 0. 0. 0. 1. 2. 1. 0. 2. 0. 1. 1. 2. 1. 2. 1. 0.\n",
      " 2. 1. 0. 0. 1. 0. 0. 1. 3. 0. 3. 0. 0. 1. 2. 1. 1. 0. 0. 0. 2. 1.]\n",
      "Accuracy: 65.86147186147187% +/- 5.95766628787512%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Original:\\n')\n",
    "print(testlabels)\n",
    "print('Fitted:\\n')\n",
    "print(classify.predict(testvectors))\n",
    "acc = cross_val_score(classify, testvectors, testlabels, cv=5)\n",
    "print('Accuracy: '+str((np.average(acc))*100)+'% +/- '+str((np.std(acc))*100)+'%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Thi should work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLPClassifier(activation='tanh', alpha=0.0001, batch_size=20, beta_1=0.9,\n",
      "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
      "       hidden_layer_sizes=(10, 20, 8, 4), learning_rate='constant',\n",
      "       learning_rate_init=0.001, max_iter=1000, momentum=0.9,\n",
      "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
      "       solver='sgd', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
      "       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "#Let's try classification with fully connected neura networks instead.\n",
    "network=MLPClassifier(hidden_layer_sizes=(10,20,8,4), solver='sgd', alpha=0.0001, activation='tanh', batch_size=20, max_iter=1000, random_state=1)\n",
    "print(network.fit(vectorize('./trainemoji.txt',model), labelize('./trainemoji.txt',model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:\n",
      "\n",
      "[2. 2. 3. 0. 0. 2. 0. 3. 1. 0. 2. 1. 2. 1. 0. 2. 0. 1. 3. 2. 2. 2. 3. 0.\n",
      " 1. 1. 0. 0. 3. 0. 0. 3. 3. 1. 3. 0. 0. 1. 3. 1. 2. 1. 1. 0. 2. 1.]\n",
      "Fitted:\n",
      "\n",
      "[2. 0. 2. 0. 0. 2. 0. 3. 1. 0. 2. 1. 2. 2. 0. 2. 0. 1. 3. 2. 1. 2. 1. 0.\n",
      " 3. 1. 0. 0. 3. 0. 0. 3. 3. 0. 3. 0. 0. 1. 3. 1. 2. 1. 1. 0. 2. 1.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/usr/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 76.7994227994228% +/- 5.370357424484229%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "print('Original:\\n')\n",
    "print(testlabels)\n",
    "print('Fitted:\\n')\n",
    "print(network.predict(testvectors))\n",
    "accn = cross_val_score(network, testvectors, testlabels, cv=5)\n",
    "print('Accuracy: '+str((np.average(accn))*100)+'% +/- '+str((np.std(accn))*100)+'%\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is number 0: ‚ô•\n",
      "This is number 1: üòû\n",
      "This is number 2: üõ´\n",
      "This is number 3: üç¥\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "print(emoji.emojize('This is number 0: :heart_suit:'))\n",
    "print(emoji.emojize('This is number 1: :disappointed_face:'))\n",
    "print(emoji.emojize('This is number 2: :airplane_departure:'))\n",
    "print(emoji.emojize('This is number 3: :fork_and_knife:'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ´ 2 The first condition of understanding a foreign country is to smell it.\n",
      "\n",
      "‚ô• 2 Those who know nothing of foreign languages know nothing of their own.\n",
      "\n",
      "üõ´ 3 Ask not what you can do for your country. Ask what's for lunch.\n",
      "\n",
      "‚ô• 0 You are, and always have been, my dream.\n",
      "\n",
      "‚ô• 0 You're my dream lover, and I couldn't picture someone better suited for me than you.\n",
      "\n",
      "üõ´ 2 Jet lag is for amateurs.\n",
      "\n",
      "‚ô• 0 My love and adoration for you only grows stronger with time. Much, much stronger.\n",
      "\n",
      "üç¥ 3 The odds of going to the store for a loaf of bread and coming out with only a loaf of bread are three billion to one.\n",
      "\n",
      "üòû 1 You still mean everything to me, but you're just not worth the fight anymore.\n",
      "\n",
      "‚ô• 0 Seeing you was love at first sight. Seeing you changed my life forever.\n",
      "\n",
      "üõ´ 2 Let your memory be your travel bag.\n",
      "\n",
      "üòû 1 Sometimes we do have to be selfless not selfish and think of what's going to be the best for the ones we love.\n",
      "\n",
      "üõ´ 2 Every dreamer knows that it is entirely possible to be homesick for a place you've never been to, perhaps more homesick than for familiar ground.\n",
      "\n",
      "üõ´ 1 Waiting for you is like waiting for rain in a drought useless and disappointing.\n",
      "\n",
      "‚ô• 0 I will never stop loving you, and I'll cherish you as long as I live.\n",
      "\n",
      "üõ´ 2 The impulse to travel is one of the hopeful symptoms of life.\n",
      "\n",
      "‚ô• 0 You're my number one priority in life. I put you above anything and everything else.\n",
      "\n",
      "üòû 1 Nothing hurts more than realizing he was everything to you and you were nothing.\n",
      "\n",
      "üç¥ 3 The trouble with eating Italian is that 5 or 6 days later, you're hungry again.\n",
      "\n",
      "üõ´ 2 Make voyages! Attempt them there's nothing else.\n",
      "\n",
      "üòû 2 Don't listen to what they say. Go see.\n",
      "\n",
      "üõ´ 2 Venice, it's temples and palaces did seem like fabrics of enchantment piled to heaven.\n",
      "\n",
      "üòû 3 Hearts can't be broken because they're made of marzipan.\n",
      "\n",
      "‚ô• 0 No one else on the planet could ever compare to you. You're pure beauty.\n",
      "\n",
      "üç¥ 1 Remember when we used to stop in the halls to talk to each other ? Well, those are the times i miss\n",
      "\n",
      "üòû 1 No guy is worth your tears and the ones who are won't make you cry.\n",
      "\n",
      "‚ô• 0 I can't imagine living a life without you. You are my reason to be.\n",
      "\n",
      "‚ô• 0 When I'm in your company, I feel like the luckiest person in the world.\n",
      "\n",
      "üç¥ 3 If you're afraid of butter, use cream.\n",
      "\n",
      "‚ô• 0 I feel like my life started on the day I met you. You're a dream.\n",
      "\n",
      "‚ô• 0 You're the only person on the planet who fulfills me. You're my other half.\n",
      "\n",
      "üç¥ 3 Wait. Why am I thinking about pizza? We're supposed to be exercising.\n",
      "\n",
      "üç¥ 3 Popcorn for breakfast! Why not? It's a grain. It's like, like, grits, but with high self-esteem.\n",
      "\n",
      "‚ô• 1 If one day you feel like crying call me. I can't promise to make you laugh but I'm willing to cry with you.\n",
      "\n",
      "üç¥ 3 Pizza tastes as good as being skinny feels.\n",
      "\n",
      "‚ô• 0 You complete me and I want to be able to do the same for you.\n",
      "\n",
      "‚ô• 0 It's you and me against the universe. We're the strongest team there is.\n",
      "\n",
      "üòû 1 Every girl has three guys in her life. The one she loves, the one she hates and the one she can't live without and in the end they're all the same guy.\n",
      "\n",
      "üç¥ 3 1 billion people in the world are chronically hungry. 1 billion people are overweight.\n",
      "\n",
      "üòû 1 Sometimes i wish i was a little girl again because bruised knees heal faster than broken hearts.\n",
      "\n",
      "üõ´ 2 I dislike feeling at home when I am abroad.\n",
      "\n",
      "üòû 1 It's hard to forget someone who gave you so much to remember.\n",
      "\n",
      "üòû 1 Trying to forget someone you love is like trying to remember someone you never knew.\n",
      "\n",
      "‚ô• 0 You make me want to make the world a better place. You're perfect.\n",
      "\n",
      "üõ´ 2 The bird dares to break the shell, then the shell breaks open and the bird can fly openly. This is the simplest principle of success. You dream, you dare and and you fly.\n",
      "\n",
      "üòû 1 I love you so much, it makes me cry because I know that when it finally happened that someone will take you away or you would have to leave. My love for you won't be enough to make you stay.\n"
     ]
    }
   ],
   "source": [
    "#Let's try to substitute communication with emojis, and make our parents proud.\n",
    "linesfile=open('./testemoji.txt', 'r', encoding='utf-8')\n",
    "lines=linesfile.readlines()\n",
    "linesfile.close()\n",
    "emojinum=int(0)\n",
    "emojistring=' '\n",
    "for i in range(len(network.predict(testvectors))):\n",
    "    emojinum=network.predict(testvectors)[i]\n",
    "    if emojinum==0:\n",
    "        emojistring=':heart_suit:'\n",
    "    elif emojinum==1:\n",
    "        emojistring=':disappointed_face:'\n",
    "    elif emojinum==2:\n",
    "        emojistring=':airplane_departure:'\n",
    "    elif emojinum==3:\n",
    "        emojistring=':fork_and_knife:'\n",
    "    print(emoji.emojize(emojistring)+' '+lines[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
